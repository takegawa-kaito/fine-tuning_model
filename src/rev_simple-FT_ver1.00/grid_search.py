import os
import sys
import yaml
import json
import itertools
import logging
from datetime import datetime
from pathlib import Path
from copy import deepcopy
from typing import Dict, List, Any, Tuple
import pandas as pd
import numpy as np
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from main import FineTuningPipeline


class GridSearchManager:
    """
    ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒç®¡ç†ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, base_config_path: str):
        """
        Args:
            base_config_path (str): ãƒ™ãƒ¼ã‚¹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.base_config_path = base_config_path
        with open(base_config_path, 'r', encoding='utf-8') as f:
            self.base_config = yaml.safe_load(f)
        
        self.grid_config = self.base_config.get('grid_search', {})
        self.results = []
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = self._setup_logger()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.results_dir = os.path.join(
            self.base_config['data_paths']['output_path'], 
            'grid_search_results',
            f"grid_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        os.makedirs(self.results_dir, exist_ok=True)
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚°è¨­å®š"""
        logger = logging.getLogger('GridSearch')
        logger.setLevel(logging.INFO)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def generate_parameter_combinations(self) -> List[Dict]:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
        
        Returns:
            List[Dict]: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã®ãƒªã‚¹ãƒˆ
        """
        if not self.grid_config.get('enable', False):
            self.logger.info("Grid search is disabled")
            return [{}]
        
        parameters = self.grid_config.get('parameters', {})
        combinations = []
        
        # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
        param_lists = {}
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        if 'optimizer' in parameters:
            param_lists['optimizer'] = parameters['optimizer']
        
        # å­¦ç¿’ç‡ï¼ˆäº‹å‰å­¦ç¿’ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
        if 'learning_rate' in parameters:
            lr_params = parameters['learning_rate']
            param_lists['pretrain_lr'] = lr_params.get('pretrain', [0.001])
            param_lists['finetune_lr'] = lr_params.get('finetune', [0.0001])
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        if 'scheduler' in parameters:
            scheduler_params = parameters['scheduler']
            param_lists['scheduler_type'] = scheduler_params.get('type', ['StepLR'])
            param_lists['cosine_T_max'] = scheduler_params.get('cosine_T_max', [100])
            param_lists['step_gamma'] = scheduler_params.get('step_gamma', [0.7])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
        if 'model' in parameters:
            model_params = parameters['model']
            param_lists['hidden_size'] = model_params.get('hidden_size', [256])
            param_lists['dropout_rate'] = model_params.get('dropout_rate', [0.15])
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚º
        if 'batch_size' in parameters:
            batch_params = parameters['batch_size']
            param_lists['pretrain_batch'] = batch_params.get('pretrain', [32])
            param_lists['finetune_batch'] = batch_params.get('finetune', [16])
        
        # çµ„ã¿åˆã‚ã›ç”Ÿæˆ
        param_names = list(param_lists.keys())
        param_values = list(param_lists.values())
        
        all_combinations = list(itertools.product(*param_values))
        
        # æœ€å¤§å®Ÿé¨“æ•°åˆ¶é™
        max_experiments = self.grid_config.get('max_experiments', 20)
        if len(all_combinations) > max_experiments:
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            np.random.seed(42)
            selected_indices = np.random.choice(
                len(all_combinations), 
                size=max_experiments, 
                replace=False
            )
            all_combinations = [all_combinations[i] for i in selected_indices]
            self.logger.warning(f"Too many combinations ({len(list(itertools.product(*param_values)))}). "
                              f"Randomly selected {max_experiments} combinations.")
        
        # è¾æ›¸å½¢å¼ã«å¤‰æ›
        for combination in all_combinations:
            param_dict = dict(zip(param_names, combination))
            combinations.append(param_dict)
        
        self.logger.info(f"Generated {len(combinations)} parameter combinations")
        return combinations
    
    def create_config_for_combination(self, base_config: Dict, params: Dict) -> Dict:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ç”¨ã®è¨­å®šã‚’ä½œæˆ
        
        Args:
            base_config (Dict): ãƒ™ãƒ¼ã‚¹è¨­å®š
            params (Dict): ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›
            
        Returns:
            Dict: æ›´æ–°ã•ã‚ŒãŸè¨­å®š
        """
        config = deepcopy(base_config)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š
        if 'optimizer' in params:
            config['pretrain']['optimizer'] = params['optimizer']
            config['finetune']['optimizer'] = params['optimizer']
        
        # å­¦ç¿’ç‡è¨­å®š
        if 'pretrain_lr' in params:
            config['pretrain']['learning_rate'] = params['pretrain_lr']
        if 'finetune_lr' in params:
            config['finetune']['learning_rate'] = params['finetune_lr']
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        if 'scheduler_type' in params:
            config['pretrain']['scheduler']['type'] = params['scheduler_type']
            config['finetune']['scheduler']['type'] = params['scheduler_type']
            
            # CosineAnnealingLRç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            if params['scheduler_type'] == 'CosineAnnealingLR':
                if 'cosine_T_max' in params:
                    config['pretrain']['scheduler']['T_max'] = params['cosine_T_max']
                    config['finetune']['scheduler']['T_max'] = params['cosine_T_max']
            
            # StepLRç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            elif params['scheduler_type'] == 'StepLR':
                if 'step_gamma' in params:
                    config['pretrain']['scheduler']['gamma'] = params['step_gamma']
                    config['finetune']['scheduler']['gamma'] = params['step_gamma']
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®š
        if 'hidden_size' in params:
            config['model']['architecture']['hidden_size'] = params['hidden_size']
        if 'dropout_rate' in params:
            config['model']['architecture']['dropout_rate'] = params['dropout_rate']
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
        if 'pretrain_batch' in params:
            config['pretrain']['batch_size'] = params['pretrain_batch']
        if 'finetune_batch' in params:
            config['finetune']['batch_size'] = params['finetune_batch']
        
        # å®Ÿé¨“åã«çµ„ã¿åˆã‚ã›æƒ…å ±ã‚’è¿½åŠ 
        param_str = "_".join([f"{k}{v}" for k, v in sorted(params.items())])
        config['experiment']['name'] = f"gridsearch_{param_str}"
        config['experiment']['description'] = f"Grid search experiment: {param_str}"
        
        return config
    
    def run_single_experiment(self, config: Dict, exp_id: int) -> Dict:
        """
        å˜ä¸€å®Ÿé¨“ã®å®Ÿè¡Œ
        
        Args:
            config (Dict): å®Ÿé¨“è¨­å®š
            exp_id (int): å®Ÿé¨“ID
            
        Returns:
            Dict: å®Ÿé¨“çµæœ
        """
        self.logger.info(f"Running experiment {exp_id}: {config['experiment']['name']}")
        
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
            temp_config_path = os.path.join(self.results_dir, f"config_exp_{exp_id}.yaml")
            with open(temp_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
            pipeline = FineTuningPipeline(temp_config_path)
            results = pipeline.run_full_pipeline()
            
            # çµæœã‚’ã¾ã¨ã‚ã‚‹
            experiment_result = {
                'experiment_id': exp_id,
                'parameters': self._extract_key_parameters(config),
                'results': {
                    'pretrain_source_rmse': results['evaluation']['pretrain']['source'].get('RMSE', None),
                    'finetune_source_rmse': results['evaluation']['finetune']['source'].get('RMSE', None),
                    'finetune_target_rmse': results['evaluation']['finetune']['target'].get('RMSE', None),
                    'pretrain_source_r2': results['evaluation']['pretrain']['source'].get('R2', None),
                    'finetune_source_r2': results['evaluation']['finetune']['source'].get('R2', None),
                    'finetune_target_r2': results['evaluation']['finetune']['target'].get('R2', None),
                },
                'experiment_directory': results['experiment_directory'],
                'status': 'success'
            }
            
            self.logger.info(f"Experiment {exp_id} completed successfully")
            self.logger.info(f"Target RMSE: {experiment_result['results']['finetune_target_rmse']:.4f}")
            
        except Exception as e:
            self.logger.error(f"Experiment {exp_id} failed: {str(e)}")
            experiment_result = {
                'experiment_id': exp_id,
                'parameters': self._extract_key_parameters(config),
                'results': {},
                'error': str(e),
                'status': 'failed'
            }
        
        return experiment_result
    
    def _extract_key_parameters(self, config: Dict) -> Dict:
        """è¨­å®šã‹ã‚‰ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        return {
            'pretrain_optimizer': config['pretrain']['optimizer'],
            'finetune_optimizer': config['finetune']['optimizer'],
            'pretrain_lr': config['pretrain']['learning_rate'],
            'finetune_lr': config['finetune']['learning_rate'],
            'pretrain_scheduler': config['pretrain']['scheduler']['type'],
            'finetune_scheduler': config['finetune']['scheduler']['type'],
            'hidden_size': config['model']['architecture']['hidden_size'],
            'dropout_rate': config['model']['architecture']['dropout_rate'],
            'pretrain_batch_size': config['pretrain']['batch_size'],
            'finetune_batch_size': config['finetune']['batch_size'],
        }
    
    def run_grid_search(self) -> List[Dict]:
        """
        ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ
        
        Returns:
            List[Dict]: å…¨å®Ÿé¨“çµæœ
        """
        self.logger.info("Starting Grid Search")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ç”Ÿæˆ
        combinations = self.generate_parameter_combinations()
        
        if not combinations or combinations == [{}]:
            self.logger.info("No grid search combinations. Running single experiment with base config.")
            pipeline = FineTuningPipeline(self.base_config_path)
            results = pipeline.run_full_pipeline()
            return [{'base_experiment': results}]
        
        # å„çµ„ã¿åˆã‚ã›ã§å®Ÿé¨“å®Ÿè¡Œ
        all_results = []
        
        # tqdmã§é€²æ—è¡¨ç¤º
        progress_bar = tqdm(
            combinations, 
            desc="Grid Search Progress",
            unit="experiment",
            ncols=100
        )
        
        for i, params in enumerate(progress_bar):
            # è¨­å®šä½œæˆ
            experiment_config = self.create_config_for_combination(self.base_config, params)
            
            # é€²æ—ãƒãƒ¼ã®èª¬æ˜ã‚’æ›´æ–°
            progress_bar.set_description(f"Exp {i+1}/{len(combinations)} - {params.get('optimizer', 'Unknown')}")
            
            # å®Ÿé¨“å®Ÿè¡Œ
            result = self.run_single_experiment(experiment_config, i)
            all_results.append(result)
            
            # ä¸­é–“çµæœä¿å­˜
            self._save_intermediate_results(all_results)
            
            # æˆåŠŸã—ãŸå®Ÿé¨“ã®æ€§èƒ½ã‚’é€²æ—ãƒãƒ¼ã«è¡¨ç¤º
            if result.get('status') == 'success' and 'results' in result:
                target_rmse = result['results'].get('finetune_target_rmse', 0)
                progress_bar.set_postfix(RMSE=f"{target_rmse:.2f}")
        
        progress_bar.close()
        
        # æœ€çµ‚çµæœä¿å­˜ãƒ»åˆ†æ
        self._save_final_results(all_results)
        self._analyze_results(all_results)
        
        return all_results
    
    def _save_intermediate_results(self, results: List[Dict]):
        """ä¸­é–“çµæœä¿å­˜"""
        results_file = os.path.join(self.results_dir, 'intermediate_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_final_results(self, results: List[Dict]):
        """æœ€çµ‚çµæœä¿å­˜"""
        # JSONä¿å­˜
        results_file = os.path.join(self.results_dir, 'grid_search_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # CSVä¿å­˜ï¼ˆæˆåŠŸã—ãŸå®Ÿé¨“ã®ã¿ï¼‰
        successful_results = [r for r in results if r.get('status') == 'success']
        if successful_results:
            df_data = []
            for result in successful_results:
                row = {
                    'experiment_id': result['experiment_id'],
                    **result['parameters'],
                    **result['results']
                }
                df_data.append(row)
            
            df = pd.DataFrame(df_data)
            csv_file = os.path.join(self.results_dir, 'grid_search_summary.csv')
            df.to_csv(csv_file, index=False)
            
            self.logger.info(f"Results saved to: {results_file}")
            self.logger.info(f"Summary saved to: {csv_file}")
    
    def _analyze_results(self, results: List[Dict]):
        """çµæœåˆ†æ"""
        successful_results = [r for r in results if r.get('status') == 'success']
        
        if not successful_results:
            self.logger.warning("No successful experiments to analyze")
            return
        
        self.logger.info("=" * 50)
        self.logger.info("GRID SEARCH ANALYSIS")
        self.logger.info("=" * 50)
        
        # æœ€è‰¯çµæœã®ç‰¹å®š
        metric = self.grid_config.get('metric_for_best', 'finetune_target_rmse')
        
        if metric in ['finetune_target_rmse', 'finetune_source_rmse', 'pretrain_source_rmse']:
            # RMSEï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
            best_result = min(successful_results, 
                            key=lambda x: x['results'].get(metric, float('inf')))
            self.logger.info(f"Best result (lowest {metric}):")
        else:
            # R2ï¼ˆå¤§ãã„ã»ã©è‰¯ã„ï¼‰
            best_result = max(successful_results, 
                            key=lambda x: x['results'].get(metric, -float('inf')))
            self.logger.info(f"Best result (highest {metric}):")
        
        self.logger.info(f"  Experiment ID: {best_result['experiment_id']}")
        self.logger.info(f"  Target RMSE: {best_result['results'].get('finetune_target_rmse', 'N/A'):.4f}")
        self.logger.info(f"  Target RÂ²: {best_result['results'].get('finetune_target_r2', 'N/A'):.4f}")
        self.logger.info(f"  Parameters:")
        for key, value in best_result['parameters'].items():
            self.logger.info(f"    {key}: {value}")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        target_rmses = [r['results'].get('finetune_target_rmse') for r in successful_results 
                       if r['results'].get('finetune_target_rmse') is not None]
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ç¶ºéº—ã«çµæœè¡¨ç¤º
        print("\n" + "="*80)
        print("ğŸ” GRID SEARCH RESULTS ANALYSIS")
        print("="*80)
        
        if target_rmses:
            print(f"ğŸ† Best Target RMSE: {np.min(target_rmses):.4f}")
            print(f"ğŸ“Š Target RMSE Statistics:")
            print(f"   Mean: {np.mean(target_rmses):.4f}")
            print(f"   Std:  {np.std(target_rmses):.4f}")
            print(f"   Min:  {np.min(target_rmses):.4f}")
            print(f"   Max:  {np.max(target_rmses):.4f}")
        
        print(f"\nâœ… Successful: {len(successful_results)}/{len(results)} experiments")
        
        # æœ€è‰¯çµæœã®è©³ç´°
        if target_rmses:
            best_result = min(successful_results, 
                            key=lambda x: x['results'].get('finetune_target_rmse', float('inf')))
            print(f"\nğŸ¯ Best Configuration:")
            print(f"   Target RMSE: {best_result['results'].get('finetune_target_rmse', 'N/A'):.4f}")
            print(f"   Target RÂ²:   {best_result['results'].get('finetune_target_r2', 'N/A'):.4f}")
            print(f"   Parameters:")
            for key, value in best_result['parameters'].items():
                print(f"     {key}: {value}")
        
        # è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        self._create_detailed_csv(successful_results)
        print("="*80)
    
    def _create_detailed_csv(self, successful_results: List[Dict]):
        """è©³ç´°ãªCSVçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        csv_data = []
        for result in successful_results:
            row = {
                'experiment_id': result['experiment_id'],
                'target_rmse': result['results'].get('finetune_target_rmse'),
                'target_r2': result['results'].get('finetune_target_r2'),
                'target_mae': result['results'].get('finetune_target_mae'),
                'source_rmse': result['results'].get('finetune_source_rmse'),
                'source_r2': result['results'].get('finetune_source_r2'),
                'pretrain_rmse': result['results'].get('pretrain_source_rmse'),
                'pretrain_r2': result['results'].get('pretrain_source_r2'),
                **result['parameters']
            }
            csv_data.append(row)
        
        df = pd.DataFrame(csv_data)
        # RMSEã§æ˜‡é †ã‚½ãƒ¼ãƒˆï¼ˆè‰¯ã„çµæœãŒä¸Šã«ï¼‰
        df = df.sort_values('target_rmse', ascending=True)
        
        csv_file = os.path.join(self.results_dir, 'detailed_results.csv')
        df.to_csv(csv_file, index=False, float_format='%.6f')
        
        print(f"ğŸ“ Detailed CSV saved: {csv_file}")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    current_dir = Path(__file__).parent
    config_path = current_dir / 'config' / 'config.yaml'
    
    if not config_path.exists():
        print(f"Config file not found: {config_path}")
        return
    
    print("=" * 60)
    print("Grid Search for Fine-Tuning Pipeline")
    print("=" * 60)
    
    try:
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–ãƒ»å®Ÿè¡Œ
        grid_manager = GridSearchManager(str(config_path))
        results = grid_manager.run_grid_search()
        
        print("\nGrid search completed successfully!")
        print(f"Results saved in: {grid_manager.results_dir}")
        
    except Exception as e:
        print(f"Error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
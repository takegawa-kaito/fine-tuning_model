# Fine-Tuning Model Configuration (ver1.00)
# FeFe (Source) → FeCu (Target) Domain Adaptation [REVERSED]

# ==================================================
# データ設定
# ==================================================

# データパス
data_paths:
  source_raw_data:
    - "C:/Users/4093050/Documents/プロセスインフォマティクス/FT/fine-tuning_model/input/raw/fefe_JF_L4mm_20250401_start_end_3000_1500_0_rev1.csv"
    - "C:/Users/4093050/Documents/プロセスインフォマティクス/FT/fine-tuning_model/input/raw/fefe_JF_L4mm_add_20250404_start_end_625_500_0_rev1.csv"
  target_raw_data:
    - "C:/Users/4093050/Documents/プロセスインフォマティクス/FT/fine-tuning_model/input/raw/fecu_JF_L4mm_20250120_start_end_150_300_0_rev2_removebaddata.csv"
    - "C:/Users/4093050/Documents/プロセスインフォマティクス/FT/fine-tuning_model/input/raw/fecu_JF_L4mm_add_20250130_start_end_625_500_0_rev2_removebaddata.csv"
  preprocessed_path: "C:/Users/4093050/Documents/プロセスインフォマティクス/FT/fine-tuning_model/input/preprocessed"
  output_path: "C:/Users/4093050/Documents/プロセスインフォマティクス/FT/fine-tuning_model/src/rev_simple-FT_ver1.00/data"

# データ前処理
preprocessing:
  source_raw_filenames:
    - "fefe_JF_L4mm_20250401_start_end_3000_1500_0_rev1.csv"
    - "fefe_JF_L4mm_add_20250404_start_end_625_500_0_rev1.csv"
  target_raw_filenames:
    - "fecu_JF_L4mm_20250120_start_end_150_300_0_rev2_removebaddata.csv"
    - "fecu_JF_L4mm_add_20250130_start_end_625_500_0_rev2_removebaddata.csv"
  source_preprocessed_filename: "target_target.csv"  # FeFe data (reversed: was target, now source)
  target_preprocessed_filename: "source_target.csv"  # FeCu data (reversed: was source, now target)
  columns: ["Power", "Velocity", "Defocus", "fz"]
  input_path: "C:\\Users\\4093050\\Documents\\プロセスインフォマティクス\\FT\\fine-tuning_model\\input\\raw"

# データ抽出
data_extraction:
  random_seed: 42
  source:
    train_ratio: 0.8
    validation_ratio: 0.2
    extraction_pattern: "random"
  target:
    learning_data_num: 256  # 学習データ数 (64/128/256/512)
    validation_ratio: 0.2
    validation_pattern: "interpolation"
  extraction_patterns:
    - name: "random"
      pattern: 1
    - name: "interpolation"
      pattern: 2
    - name: "extrapolation"
      pattern: 3

# ==================================================
# モデル設定
# ==================================================

# 入出力データ
model_data:
  features: ["Power", "Velocity"]
  target: "fz"
  standardization:
    apply: true
    method: "StandardScaler"

# アーキテクチャ
model:
  name: "SimpleRegressor"
  architecture:
    input_dim: 2
    hidden_size: 256
    output_dim: 1
    activation: "GELU"
    dropout_rate: 0.15

# ==================================================
# 学習設定
# ==================================================

# 事前学習 (Source Domain)
pretrain:
  epochs: 200
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "Adam"
  loss_function: "MSELoss"
  scheduler:
    use: true
    type: "StepLR"
    step_size: 50
    gamma: 0.5
    T_max: 100  # CosineAnnealingLR用
  early_stopping:
    patience: 20
    min_delta: 0.001
  save_best_model: true

# ファインチューニング (Target Domain)
finetune:
  epochs: 100
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 0.00005
  optimizer: "AdamW"
  loss_function: "MSELoss"
  scheduler:
    use: true
    type: "CosineAnnealingLR"
    step_size: 30
    gamma: 0.7
    T_max: 50  # CosineAnnealingLR用
  early_stopping:
    patience: 15
    min_delta: 0.0005
  freeze_layers:
    enable: false
    freeze_until_layer: 0
  save_best_model: true

# ==================================================
# 実験・評価設定
# ==================================================

# 評価
evaluation:
  metrics: ["RMSE", "MAE", "R2"]
  save_predictions: true
  plot_results: true
  plot_loss_curves: true
  comparison_analysis: true

# グリッドサーチ
grid_search:
  enable: true
  max_experiments: 6  # さらに実験数を減らして高速化
  parameters:
    optimizer: ["Adam", "AdamW"]
    learning_rate:
      pretrain: [0.001]
      finetune: [0.0001, 0.0005]
    scheduler:
      type: ["StepLR", "CosineAnnealingLR"]
      cosine_T_max: [50, 100, 150]
      step_gamma: [0.5, 0.7, 0.9]
    model:
      hidden_size: [128, 256, 512]
      dropout_rate: [0.1, 0.15, 0.2]
    batch_size:
      pretrain: [16, 32, 64]
      finetune: [8, 16, 32]
  save_results: true
  save_best_only: true
  metric_for_best: "target_rmse"

# 実験情報
experiment:
  name: "fecu_to_fefe_finetune"
  description: "FeCu source data pretraining + FeFe target data finetuning"
  random_seed: 42
  device: "auto"
  verbose: true
  save_logs: true

# ログ
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  save_to_file: true
  log_file: "training.log"